<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Agora Agent Demo — Video + Live Transcripts</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
    body { font-family: Inter, system-ui, sans-serif; background:#071024; color:#dff0ff; padding:18px; }
    .controls { display:flex; gap:8px; align-items:center; margin-bottom:12px; }
    input { padding:8px; border-radius:8px; border:1px solid #123; background:#081224; color:#dff0ff; }
    button { padding:8px 12px; border-radius:8px; border:none; background:#06b6d4; color:#022; cursor:pointer; }
    #grid { display:grid; grid-template-columns:1fr 1fr; gap:12px; height:46vh; }
    .box { background:#000; border-radius:8px; overflow:hidden; position:relative; border:1px solid #102434; padding:10px; box-sizing:border-box; }
    .label { position:absolute; left:8px; top:8px; background:#0009; padding:4px 8px; border-radius:6px; font-size:12px; color:#9fd6e6; }
    #log { margin-top:12px; background:#051020; padding:10px; border-radius:8px; height:120px; overflow:auto; font-size:12px; color:#bfe6f7; border:1px solid #113244; white-space:pre-wrap; }
    .small { font-size:13px; color:#9fbcd1; margin-left:12px; }
    .transcript { font-size:14px; line-height:1.4; color:#dff0ff; background:linear-gradient(180deg,#021426, #031a2a); padding:8px; border-radius:6px; max-height:40vh; overflow:auto; }
    .row { display:flex; gap:8px; align-items:center; }
    .user-bubble, .ai-bubble { padding:8px 10px; border-radius:10px; margin-bottom:8px; max-width:90%; }
    .user-bubble { background:#052b3a; color:#bfe6f7; align-self:flex-end; }
    .ai-bubble { background:#0b2b18; color:#bff7d6; align-self:flex-start; }
    .transcript-col { display:flex; flex-direction:column; gap:6px; }
    audio { width:100%; margin-top:6px; }
    .status { font-size:12px; color:#9fbcd1; margin-left:8px; }
  </style>
</head>
<body>
  <h2>Agora Conversational AI — Demo (Local) — Live Transcripts</h2>

  <div class="controls">
    <input id="channel" placeholder="channel name" value="test-room" />
    <button id="joinBtn">Join</button>
    <button id="leaveBtn" disabled>Leave</button>
    <div class="small">Backend: <strong>http://localhost:8000</strong></div>
    <div class="status" id="recStatus">STT: <em>stopped</em></div>
  </div>

  <div id="grid">
    <div class="box">
      <div class="label">Local</div>
      <div id="local"></div>
    </div>

    <div class="box">
      <div class="label">Remote / Agent</div>
      <div id="remote"></div>
    </div>
  </div>

  <div style="display:grid; grid-template-columns: 1fr 1fr; gap:12px; margin-top:12px;">
    <div class="box">
      <div class="label" style="top:6px">Conversation</div>
      <div class="transcript" id="conversation">
        <!-- user and AI bubbles appended here -->
      </div>
    </div>

    <div class="box">
      <div class="label" style="top:6px">Debug / Raw Logs</div>
      <pre id="log"></pre>
    </div>
  </div>

  <!-- Agora SDK (correct CDN loaded before using AgoraRTC) -->
  <script src="https://download.agora.io/sdk/release/AgoraRTC_N.js"></script>

  <script>
    const LOG = document.getElementById("log");
    function uiLog(...args) {
      const t = new Date().toLocaleTimeString();
      const text = args.map(a => {
        if (typeof a === "object") {
          try { return JSON.stringify(a, null, 2); } catch (e) { return String(a); }
        }
        return String(a);
      }).join(" ");
      LOG.textContent = t + " — " + text + "\n" + LOG.textContent;
      console.log(...args);
    }

    // conversation UI
    const CONV = document.getElementById("conversation");
    function appendUserText(txt) {
      const div = document.createElement("div");
      div.className = "user-bubble";
      div.textContent = txt;
      CONV.appendChild(div);
      CONV.scrollTop = CONV.scrollHeight;
    }
    function appendAiText(txt, audioSrc = null) {
      const wrapper = document.createElement("div");
      wrapper.style.display = "flex";
      wrapper.style.flexDirection = "column";
      wrapper.style.alignItems = "flex-start";
      const div = document.createElement("div");
      div.className = "ai-bubble";
      div.textContent = txt;
      wrapper.appendChild(div);
      if (audioSrc) {
        // if we have a playble audio source (url or data:), add audio element
        try {
          const a = document.createElement("audio");
          a.controls = true;
          a.src = audioSrc;
          wrapper.appendChild(a);
          // autoplay if allowed
          a.play().catch(e => uiLog("Audio autoplay blocked:", e.message));
        } catch (e) {
          uiLog("Failed to attach audio:", e);
        }
      }
      CONV.appendChild(wrapper);
      CONV.scrollTop = CONV.scrollHeight;
    }

    // ← CHANGED: backend port set to 8000
    const backend = "http://localhost:8000";
    const joinBtn = document.getElementById("joinBtn");
    const leaveBtn = document.getElementById("leaveBtn");
    const channelInput = document.getElementById("channel");
    const localContainer = document.getElementById("local");
    const remoteContainer = document.getElementById("remote");
    const recStatus = document.getElementById("recStatus");

    // Agora client + local tracks
    let client = null;
    let localTracks = { audioTrack: null, videoTrack: null };
    let agentEventSource = null;

    // SpeechRecognition for user STT
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition || null;
    let recognizer = null;
    let isRecognizing = false;

    function initSpeechRecognition() {
      if (!SpeechRecognition) {
        uiLog("SpeechRecognition not supported in this browser.");
        recStatus.innerHTML = 'STT: <em>unsupported</em>';
        return;
      }
      if (recognizer) return;

      recognizer = new SpeechRecognition();
      recognizer.continuous = true;
      recognizer.interimResults = true;
      recognizer.lang = 'en-US';

      recognizer.onstart = () => {
        isRecognizing = true;
        recStatus.innerHTML = 'STT: <em>listening</em>';
        uiLog("SpeechRecognition started");
      };

      recognizer.onerror = (ev) => {
        uiLog("SpeechRecognition error:", ev.error || ev);
      };

      recognizer.onend = () => {
        isRecognizing = false;
        recStatus.innerHTML = 'STT: <em>stopped</em>';
        uiLog("SpeechRecognition ended");
        // If still joined, auto-restart (resilient)
        if (joinBtn.disabled) {
          setTimeout(() => {
            try { recognizer.start(); } catch (e) { uiLog("Restart SR failed:", e.message); }
          }, 500);
        }
      };

      let lastFinal = "";
      recognizer.onresult = (event) => {
        let interim = "";
        let finalTranscript = lastFinal || "";
        for (let i = event.resultIndex; i < event.results.length; ++i) {
          const res = event.results[i];
          if (res.isFinal) {
            finalTranscript += res[0].transcript;
            // show final transcript as a new user bubble and clear lastFinal
            appendUserText(finalTranscript.trim());
            uiLog("User (final):", finalTranscript.trim());
            lastFinal = "";
            // Optionally you could send the user transcript to backend here
            finalTranscript = "";
          } else {
            interim += res[0].transcript;
          }
        }
        // show interim as a temporary UI element (replace last interim bubble)
        // we'll keep it simple: show interim in console + as a small appended bubble (not final)
        if (interim && interim.length) {
          uiLog("User (interim):", interim);
        }
      };
    }

    function startRecognition() {
      if (!SpeechRecognition) return;
      try {
        initSpeechRecognition();
        if (recognizer && !isRecognizing) {
          recognizer.start();
        }
      } catch (e) {
        uiLog("Failed to start SpeechRecognition:", e);
      }
    }

    function stopRecognition() {
      if (!recognizer) return;
      try {
        recognizer.stop();
      } catch (e) {
        uiLog("Failed to stop SpeechRecognition:", e);
      }
    }

    function initClient() {
      if (client) return;
      client = AgoraRTC.createClient({ mode: "rtc", codec: "vp8" });

      client.on("user-published", async (user, mediaType) => {
        uiLog("user-published:", user.uid, mediaType);
        await client.subscribe(user, mediaType);
        if (mediaType === "video") {
          remoteContainer.innerHTML = "";
          const div = document.createElement("div");
          div.style.width = "100%";
          div.style.height = "100%";
          remoteContainer.appendChild(div);
          try { user.videoTrack.play(div); }
          catch(e) { uiLog("Error playing remote video:", e); }
        }
        if (mediaType === "audio") {
          try { user.audioTrack.play(); } catch(e) { uiLog("Error playing remote audio:", e); }
        }
      });

      client.on("user-unpublished", (user) => {
        uiLog("user-unpublished:", user.uid);
        const el = document.getElementById(`remote-player-${user.uid}`);
        if (el && el.parentNode) el.parentNode.removeChild(el);
      });

      client.on("user-left", (user) => {
        uiLog("user-left:", user.uid);
      });

      uiLog("Agora client initialized");
    }

    async function safeFetchJoin(channel) {
      try {
        const res = await fetch(`${backend}/api/agent/join`, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ channel })
        });

        // read raw text first so we can handle non-json errors
        const raw = await res.text();
        uiLog("raw join response status:", res.status);
        uiLog(raw ? (raw.length > 1000 ? raw.slice(0, 1000) + " ...[truncated]" : raw) : "(empty body)");

        if (!res.ok) {
          // Try to parse JSON detail if present
          try {
            const parsed = raw ? JSON.parse(raw) : null;
            uiLog("server error detail:", parsed);
            alert("Server error: " + (parsed?.detail || JSON.stringify(parsed) || res.status));
          } catch (e) {
            alert("Server returned non-JSON error. See debug log.");
          }
          return null;
        }

        // parse JSON safely
        let json = null;
        try {
          json = raw ? JSON.parse(raw) : null;
        } catch (e) {
          uiLog("Failed to parse JSON from server:", e, raw);
          alert("Malformed JSON from server. See debug log.");
          return null;
        }
        // Log the parsed JSON to UI + console (explicit)
        uiLog("AI JOIN RESPONSE (parsed):", json);
        console.log("AI JOIN RESPONSE (parsed):", json);
        return json;
      } catch (err) {
        uiLog("Network/fetch error:", err);
        alert("Network error: " + (err.message || err));
        return null;
      }
    }

    function extractTextFromPayload(obj) {
      // Best-effort extraction: search common fields for text
      if (!obj) return null;
      if (typeof obj === "string") return obj;
      // try direct fields
      const candidates = ["message", "text", "reply", "content", "transcript", "partial", "delta", "output"];
      for (const c of candidates) {
        if (obj[c]) {
          if (typeof obj[c] === "string") return obj[c];
          if (typeof obj[c] === "object" && obj[c].text) return obj[c].text;
        }
      }
      // nested: events or response
      if (Array.isArray(obj.events)) {
        for (const ev of obj.events) {
          const t = extractTextFromPayload(ev);
          if (t) return t;
        }
      }
      if (obj.response && obj.response.choices) {
        // handle streaming-like structures: choices[].delta.content or choices[].message.content
        for (const ch of obj.response.choices) {
          if (ch.delta && ch.delta.content) return ch.delta.content;
          if (ch.message && ch.message.content) return ch.message.content;
        }
      }
      // fallback: if object has text-like keys anywhere, try to find them recursively (shallow)
      for (const k of Object.keys(obj)) {
        if (typeof obj[k] === "string" && obj[k].length < 5000 && obj[k].split(" ").length > 0) {
          // heuristics: avoid huge base64 strings
          if (!/^data:audio|^https?:\/\/.*\.(mp3|wav|ogg)/i.test(obj[k])) return obj[k];
        }
      }
      return null;
    }

    function findAudioSrcInPayload(obj) {
      if (!obj) return null;
      // look for base64 audio or url fields
      const keys = ["audio", "audio_base64", "audio_b64", "audio_url", "tts_audio", "audioUri", "audio_url"];
      for (const k of keys) {
        if (obj[k]) {
          const v = obj[k];
          if (typeof v === "string") {
            // if it's base64 without data: prefix, try to detect length and build data: url
            if (/^[A-Za-z0-9+/=]+\s*$/.test(v) && v.length > 200) {
              // assume mp3
              return `data:audio/mp3;base64,${v}`;
            }
            if (v.startsWith("data:audio/")) return v;
            if (v.startsWith("http://") || v.startsWith("https://")) return v;
          }
        }
      }
      // sometimes nested: check properties
      if (obj.properties) {
        return findAudioSrcInPayload(obj.properties);
      }
      return null;
    }

    function attachAndLogAgentEventsFromPayload(payload) {
      try {
        if (!payload) return;

        // 1) events array
        if (Array.isArray(payload.events) && payload.events.length) {
          uiLog("payload.events found — logging each event:");
          payload.events.forEach((ev, i) => {
            uiLog(`events[${i}]`, ev);
            console.log("AI EVENT:", ev);

            // extract text and audio and display
            const text = extractTextFromPayload(ev);
            const audioSrc = findAudioSrcInPayload(ev);
            if (text) appendAiText(text, audioSrc);
          });
        }

        // 2) simple message property(s)
        const text = extractTextFromPayload(payload);
        const audioSrc = findAudioSrcInPayload(payload);
        if (text) {
          uiLog("payload message/text:", text);
          appendAiText(text, audioSrc);
        }

        // 3) SSE / stream URL — attempt to open an EventSource if provided
        const sseUrl = payload.sse_url || payload.stream_url || payload.sse || payload.stream || payload.ai_stream;
        if (sseUrl && typeof EventSource !== "undefined") {
          openAgentEventSource(sseUrl);
        }
      } catch (e) {
        uiLog("Error while attaching/logging payload events:", e);
      }
    }

    function openAgentEventSource(url) {
      try {
        // close existing if present
        if (agentEventSource) {
          try { agentEventSource.close(); } catch(e) {}
          agentEventSource = null;
        }

        uiLog("Opening EventSource to:", url);
        console.log("Opening EventSource to:", url);
        agentEventSource = new EventSource(url);

        agentEventSource.onopen = () => uiLog("EventSource open:", url);
        agentEventSource.onerror = (err) => uiLog("EventSource error:", err);

        agentEventSource.onmessage = (ev) => {
          // try to parse JSON payload if possible
          let data = ev.data;
          try { data = JSON.parse(ev.data); } catch (e) { /* leave raw string */ }
          uiLog("EventSource message:", data);
          console.log("EventSource message:", data);

          // display AI text / audio if present
          const text = extractTextFromPayload(data);
          const audioSrc = findAudioSrcInPayload(data);
          if (text) appendAiText(text, audioSrc);
        };

        // example: some SSE streams include event names other than 'message'
        agentEventSource.addEventListener("ai", e => {
          let d = e.data;
          try { d = JSON.parse(e.data); } catch (_) {}
          uiLog("EventSource [ai]:", d);
          console.log("EventSource [ai]:", d);
          const text = extractTextFromPayload(d);
          const audioSrc = findAudioSrcInPayload(d);
          if (text) appendAiText(text, audioSrc);
        });
      } catch (e) {
        uiLog("Failed to open EventSource:", e);
      }
    }

    async function join() {
      const channel = channelInput.value.trim();
      if (!channel) return alert("Enter a channel name");

      joinBtn.disabled = true;
      try {
        initClient();

        const resp = await safeFetchJoin(channel);
        if (!resp) { joinBtn.disabled = false; return; }

        const payload = resp.data || resp; // server returns {data: r.data} or direct payload
        uiLog("join payload:", payload);
        console.log("JOIN PAYLOAD:", payload);

        // log any events/messages found in the payload and display them
        attachAndLogAgentEventsFromPayload(payload);

        // Extract appId / token / channel — adjust if your server returns different keys
        const appId = payload.appid || payload.app_id || payload.app || payload.appId || payload.properties?.appid || "";
        const token = payload.token || payload.rtcToken || payload.rtc_token || null;
        const channelName = payload.channel || channel;

        if (!appId || appId === "MOCK_APPID") {
          // In mock mode use a lightweight fallback (no real RTC join).
          uiLog("Server returned mock response — publishing local preview only.");
          const [micTrack, camTrack] = await AgoraRTC.createMicrophoneAndCameraTracks();
          localTracks.audioTrack = micTrack; localTracks.videoTrack = camTrack;

          localContainer.innerHTML = "";
          const div = document.createElement("div"); div.style.width="100%"; div.style.height="100%";
          localContainer.appendChild(div);
          camTrack.play(div);

          // start local speech recognition to capture user's speech (in mock too)
          startRecognition();

          joinBtn.disabled = true;
          leaveBtn.disabled = false;
          return;
        }

        // Join the channel on Agora RTC
        await client.join(appId, channelName, token || null, null);
        uiLog("Joined Agora RTC:", channelName);

        const [microphoneTrack, cameraTrack] = await AgoraRTC.createMicrophoneAndCameraTracks();
        localTracks.audioTrack = microphoneTrack; localTracks.videoTrack = cameraTrack;

        // show local preview
        localContainer.innerHTML = "";
        const localDiv = document.createElement("div"); localDiv.style.width="100%"; localDiv.style.height="100%";
        localContainer.appendChild(localDiv);
        cameraTrack.play(localDiv);

        await client.publish([microphoneTrack, cameraTrack]);
        uiLog("Published local tracks");

        // Start speech recognition (STT) so we can print user's text on screen
        startRecognition();

        joinBtn.disabled = true;
        leaveBtn.disabled = false;
      } catch (err) {
        uiLog("Join error:", err);
        alert("Join error: " + (err.message || err));
        joinBtn.disabled = false;
      }
    }

    async function leave() {
      try {
        stopRecognition();

        if (localTracks.audioTrack) { try{ localTracks.audioTrack.stop(); localTracks.audioTrack.close(); } catch(e){} localTracks.audioTrack = null; }
        if (localTracks.videoTrack) { try{ localTracks.videoTrack.stop(); localTracks.videoTrack.close(); } catch(e){} localTracks.videoTrack = null; }
        await client?.leave();
        localContainer.innerHTML = "";
        remoteContainer.innerHTML = "";
        uiLog("Left channel");
      } catch (err) {
        uiLog("Leave error:", err);
      } finally {
        joinBtn.disabled = false;
        leaveBtn.disabled = true;
        // close SSE if open
        if (agentEventSource) {
          try { agentEventSource.close(); } catch(e) {}
          agentEventSource = null;
          uiLog("Closed agent EventSource");
        }
      }
    }

    joinBtn.addEventListener("click", join);
    leaveBtn.addEventListener("click", leave);

    // Quick health check ping to backend
    (async () => {
      try {
        const res = await fetch(`${backend}/health`);
        // some /health endpoints return non-json; try safe parse
        let h = null;
        try { h = await res.json(); } catch(e) { h = await res.text(); }
        uiLog("Backend health:", h);
      } catch (e) {
        uiLog("Backend health check failed (is server running?):", e);
      }
    })();
  </script>
</body>
</html>
